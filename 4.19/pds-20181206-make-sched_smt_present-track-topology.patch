From 55fdf19c03c121144717c95e9b0b177cf1cb883b Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Thu, 6 Dec 2018 15:52:42 +0000
Subject: [PATCH] pds: [Sync] c5511d03ec09 sched/smt: Make sched_smt_present
 track topology

---
 kernel/sched/pds.c | 16 ++++++++++++++++
 1 file changed, 16 insertions(+)

diff --git a/kernel/sched/pds.c b/kernel/sched/pds.c
index 0421a050d61c..cddf591a1603 100644
--- a/kernel/sched/pds.c
+++ b/kernel/sched/pds.c
@@ -190,6 +190,7 @@ DEFINE_PER_CPU(cpumask_t *, sched_cpu_llc_start_mask);
 DEFINE_PER_CPU(cpumask_t *, sched_cpu_affinity_chk_end_masks);
 
 #ifdef CONFIG_SCHED_SMT
+DEFINE_STATIC_KEY_FALSE(sched_smt_present);
 DEFINE_PER_CPU(int, sched_sibling_cpu);
 
 static cpumask_t sched_cpu_sg_idle_mask ____cacheline_aligned_in_smp;
@@ -5853,6 +5854,13 @@ int sched_cpu_activate(unsigned int cpu)
 	struct rq *rq = cpu_rq(cpu);
 	unsigned long flags;
 
+#ifdef CONFIG_SCHED_SMT
+	/*
+	 * When going up, increment the number of cores with SMT present.
+	 */
+	if (cpumask_weight(cpu_smt_mask(cpu)) == 2)
+		static_branch_inc_cpuslocked(&sched_smt_present);
+#endif
 	set_cpu_active(cpu, true);
 
 	if (sched_smp_initialized)
@@ -5888,6 +5896,14 @@ int sched_cpu_deactivate(unsigned int cpu)
 	 */
 	synchronize_rcu_mult(call_rcu, call_rcu_sched);
 
+#ifdef CONFIG_SCHED_SMT
+	/*
+	 * When going down, decrement the number of cores with SMT present.
+	 */
+	if (cpumask_weight(cpu_smt_mask(cpu)) == 2)
+		static_branch_dec_cpuslocked(&sched_smt_present);
+#endif
+
 	if (!sched_smp_initialized)
 		return 0;
 
-- 
2.18.1

