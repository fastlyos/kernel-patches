From e7b81af035ddc1323090d86350627881fcf9b1b0 Mon Sep 17 00:00:00 2001
From: Jens Axboe <axboe@fb.com>
Date: Sun, 17 Apr 2016 12:52:10 -0600
Subject: [PATCH] blk-wb: updates

- Use a rolling timer to track state
- Only limit depths within a window of scaling, jump back to resetting
  when it behaves nicely.
- Use 100ms window for both rotational and non-rotational
- Track inflight sync request latency. If a single request takes longer
  than a window to complete, count is as the mininum and scale.

Signed-off-by: Jens Axboe <axboe@fb.com>
---
 block/blk-core.c     |   3 +
 block/blk-mq.c       |   3 +
 block/blk-settings.c |   3 +-
 block/blk-sysfs.c    |  16 +--
 block/blk-wb.c       | 289 +++++++++++++++++++++++++++++++--------------------
 block/blk-wb.h       |  10 +-
 6 files changed, 201 insertions(+), 123 deletions(-)

diff --git a/block/blk-core.c b/block/blk-core.c
index e55a280..ead81c8 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -1400,6 +1400,7 @@ void blk_requeue_request(struct request_queue *q, struct request *rq)
 	blk_delete_timer(rq);
 	blk_clear_rq_complete(rq);
 	trace_block_rq_requeue(q, rq);
+	blk_wb_requeue(q->rq_wb, rq);
 
 	if (rq->cmd_flags & REQ_QUEUED)
 		blk_queue_end_tag(q, rq);
@@ -2530,6 +2531,7 @@ void blk_start_request(struct request *req)
 	blk_dequeue_request(req);
 
 	req->issue_time = ktime_to_ns(ktime_get());
+	blk_wb_issue(req->q->rq_wb, req);
 
 	/*
 	 * We are now handing the request to the hardware, initialize
@@ -2766,6 +2768,7 @@ void blk_finish_request(struct request *req, int error)
 		blk_unprep_request(req);
 
 	blk_account_io_done(req);
+	blk_wb_done(req->q->rq_wb, req);
 
 	if (req->end_io)
 		req->end_io(req, error);
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 52c2bd5..5003232 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -309,6 +309,7 @@ EXPORT_SYMBOL_GPL(blk_mq_free_request);
 inline void __blk_mq_end_request(struct request *rq, int error)
 {
 	blk_account_io_done(rq);
+	blk_wb_done(rq->q->rq_wb, rq);
 
 	if (rq->end_io) {
 		rq->end_io(rq, error);
@@ -418,6 +419,7 @@ void blk_mq_start_request(struct request *rq)
 		rq->next_rq->resid_len = blk_rq_bytes(rq->next_rq);
 
 	rq->issue_time = ktime_to_ns(ktime_get());
+	blk_wb_issue(q->rq_wb, rq);
 
 	blk_add_timer(rq);
 
@@ -454,6 +456,7 @@ static void __blk_mq_requeue_request(struct request *rq)
 	struct request_queue *q = rq->q;
 
 	trace_block_rq_requeue(q, rq);
+	blk_wb_requeue(q->rq_wb, rq);
 
 	if (test_and_clear_bit(REQ_ATOM_STARTED, &rq->atomic_flags)) {
 		if (q->dma_drain_size && blk_rq_bytes(rq))
diff --git a/block/blk-settings.c b/block/blk-settings.c
index 9056594..bf23825 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -841,8 +841,9 @@ EXPORT_SYMBOL_GPL(blk_queue_flush_queueable);
 void blk_set_queue_depth(struct request_queue *q, unsigned int depth)
 {
 	q->queue_depth = depth;
+
 	if (q->rq_wb)
-		blk_wb_update_limits(q->rq_wb, depth);
+		blk_wb_update_limits(q->rq_wb);
 }
 EXPORT_SYMBOL(blk_set_queue_depth);
 
diff --git a/block/blk-sysfs.c b/block/blk-sysfs.c
index 177aca8..2f34a5a 100644
--- a/block/blk-sysfs.c
+++ b/block/blk-sysfs.c
@@ -350,17 +350,17 @@ static ssize_t queue_poll_store(struct request_queue *q, const char *page,
 
 static ssize_t queue_wb_stats_show(struct request_queue *q, char *page)
 {
-	struct rq_wb *wb = q->rq_wb;
+	struct rq_wb *rwb = q->rq_wb;
 
-	if (!q->rq_wb)
+	if (!rwb)
 		return -EINVAL;
 
 	return sprintf(page, "background=%d, normal=%d, max=%d, inflight=%d,"
-				" wait=%d, bdp_wait=%d\n", wb->wb_background,
-					wb->wb_normal, wb->wb_max,
-					atomic_read(&wb->inflight),
-					waitqueue_active(&wb->wait),
-					atomic_read(wb->bdp_wait));
+				" wait=%d, bdp_wait=%d\n", rwb->wb_background,
+					rwb->wb_normal, rwb->wb_max,
+					atomic_read(&rwb->inflight),
+					waitqueue_active(&rwb->wait),
+					atomic_read(rwb->bdp_wait));
 }
 
 static ssize_t queue_wb_perc_show(struct request_queue *q, char *page)
@@ -387,7 +387,7 @@ static ssize_t queue_wb_perc_store(struct request_queue *q, const char *page,
 		return -EINVAL;
 
 	q->rq_wb->perc = perc;
-	blk_wb_update_limits(q->rq_wb, blk_queue_depth(q));
+	blk_wb_update_limits(q->rq_wb);
 	return ret;
 }
 
diff --git a/block/blk-wb.c b/block/blk-wb.c
index 5cb26f5..daa0c6c 100644
--- a/block/blk-wb.c
+++ b/block/blk-wb.c
@@ -3,15 +3,11 @@
  *
  * Copyright (C) 2016 Jens Axboe
  *
- * Things that need changing:
+ * Things that (may) need changing:
  *
  *	- Different scaling of background/normal/high priority writeback.
  *	  We may have to violate guarantees for max.
  *	- We can have mismatches between the stat window and our window.
- *	- We have outlier reads that exceed min, and we scale back. Better
- *	  check for whether writes happened at the same time, or whether
- *	  it was just the device being in a lower power state and hence
- *	  being slower for the initial IO.
  *
  */
 #include <linux/kernel.h>
@@ -29,11 +25,13 @@ enum {
 	RWB_MAX_DEPTH	= 64,
 
 	/*
-	 * 10msec window for non-rotational, 100ms for rotational
+	 * 100msec window
 	 */
-	RWB_ROT_END_INC		= 10 * 1000 * 1000ULL,
-	RWB_NONROT_END_INC	= 100 * 1000 * 1000ULL,
+	RWB_WINDOW_NSEC		= 100 * 1000 * 1000ULL,
 
+	/*
+	 * Disregard stats if we don't meet these minimums
+	 */
 	RWB_MIN_WRITE_SAMPLES	= 3,
 	RWB_MIN_READ_SAMPLES	= 1,
 
@@ -49,6 +47,47 @@ static inline bool rwb_enabled(struct rq_wb *rwb)
 	return rwb && rwb->wb_normal != 0;
 }
 
+/*
+ * Increment 'v', if 'v' is below 'below'. Returns true if we succeeded,
+ * false if 'v' + 1 would be bigger than 'below'.
+ */
+static bool atomic_inc_below(atomic_t *v, int below)
+{
+	int cur = atomic_read(v);
+
+	for (;;) {
+		int old;
+
+		if (cur >= below)
+			return false;
+		old = atomic_cmpxchg(v, cur, cur + 1);
+		if (old == cur)
+			break;
+		cur = old;
+	}
+
+	return true;
+}
+
+static void print_lat(struct blk_rq_stat *stat, unsigned long maxlat)
+{
+	printk(KERN_ERR "blk-wb: %lu > %lu\n", (unsigned long) stat[0].min, maxlat);
+	printk(KERN_ERR "   read lat: mean=%llu, min=%llu, max=%llu, samples=%llu\n",
+			stat[0].mean, stat[0].min, stat[0].max, stat[0].nr_samples);
+	printk(KERN_ERR "  write lat: mean=%llu, min=%llu, max=%llu, samples=%llu\n",
+			stat[1].mean, stat[1].min, stat[1].max, stat[1].nr_samples);
+}
+
+static void wb_timestamp(struct rq_wb *rwb, unsigned long *var)
+{
+	if (rwb_enabled(rwb)) {
+		const unsigned long cur = jiffies;
+
+		if (cur != *var)
+			*var = cur;
+	}
+}
+
 void __blk_wb_done(struct rq_wb *rwb)
 {
 	int inflight, limit = rwb->wb_normal;
@@ -83,53 +122,31 @@ void __blk_wb_done(struct rq_wb *rwb)
 	}
 }
 
-static void wb_timestamp(struct rq_wb *rwb, unsigned long *var)
-{
-	if (rwb_enabled(rwb)) {
-		const unsigned long cur = jiffies;
-
-		if (cur != *var)
-			*var = cur;
-	}
-}
-
 /*
  * Called on completion of a request. Note that it's also called when
  * a request is merged, when the request gets freed.
  */
 void blk_wb_done(struct rq_wb *rwb, struct request *rq)
 {
-	if (!(rq->cmd_flags & REQ_BUF_INFLIGHT))
+	if (!(rq->cmd_flags & REQ_BUF_INFLIGHT)) {
+		if (rwb->sync_rq == rq) {
+			rwb->sync_issue = 0;
+			rwb->sync_rq = NULL;
+		}
+
 		wb_timestamp(rwb, &rwb->last_comp);
-	else
+	} else {
+		WARN_ON_ONCE(rq == rwb->sync_rq);
 		__blk_wb_done(rwb);
-}
-
-/*
- * Increment 'v', if 'v' is below 'below'. Returns true if we succeeded,
- * false if 'v' + 1 would be bigger than 'below'.
- */
-static bool atomic_inc_below(atomic_t *v, int below)
-{
-	int cur = atomic_read(v);
-
-	for (;;) {
-		int old;
-
-		if (cur >= below)
-			return false;
-		old = atomic_cmpxchg(v, cur, cur + 1);
-		if (old == cur)
-			break;
-		cur = old;
+		rq->cmd_flags &= ~REQ_BUF_INFLIGHT;
 	}
-
-	return true;
 }
 
-static void calc_wb_limits(struct rq_wb *rwb, unsigned int depth)
+static void calc_wb_limits(struct rq_wb *rwb)
 {
-	depth = min_t(unsigned int, RWB_MAX_DEPTH, depth);
+	unsigned int depth;
+
+	depth = min_t(unsigned int, RWB_MAX_DEPTH, blk_queue_depth(rwb->q));
 
 	/*
 	 * Full perf writes are max 'perc' percentage of the depth
@@ -149,22 +166,33 @@ static bool inline stat_sample_valid(struct blk_rq_stat *stat)
 	 * that it's writes impacting us, and not just some sole read on
 	 * a device that is in a lower power state.
 	 */
-	return stat[0].nr_samples >= RWB_MIN_READ_SAMPLES &&
+	return stat[0].nr_samples >= 1 &&
 		stat[1].nr_samples >= RWB_MIN_WRITE_SAMPLES;
 }
 
-static bool lat_exceeded(struct rq_wb *rwb, struct blk_rq_stat *stat)
+static u64 rwb_sync_issue_lat(struct rq_wb *rwb)
 {
-	unsigned long maxlat;
+	u64 now, issue = ACCESS_ONCE(rwb->sync_issue);
 
-	if (!stat_sample_valid(stat))
-		return false;
+	if (!issue || !rwb->sync_rq)
+		return 0;
 
-	/*
-	 * We can't do anything with a max of 1, so just return
-	 */
-	if (rwb->wb_max <= 1)
-		return false;
+	now = ktime_to_ns(ktime_get());
+	return now - issue;
+}
+
+enum {
+	LAT_OK,
+	LAT_UNKNOWN,
+	LAT_EXCEEDED,
+};
+
+static int __latency_exceeded(struct rq_wb *rwb, struct blk_rq_stat *stat)
+{
+	u64 maxlat, thislat;
+
+	if (!stat_sample_valid(stat))
+		return LAT_UNKNOWN;
 
 	if (blk_queue_nonrot(rwb->q))
 		maxlat = RWB_NONROT_LAT;
@@ -172,78 +200,87 @@ static bool lat_exceeded(struct rq_wb *rwb, struct blk_rq_stat *stat)
 		maxlat = RWB_ROT_LAT;
 
 	if (stat[0].min > maxlat) {
-		printk(KERN_ERR "blk-wb: %lu > %lu\n", (unsigned long) stat[0].min, (unsigned long) maxlat);
-		printk(KERN_ERR "   read lat: mean=%llu, min=%llu, max=%llu, samples=%llu\n", stat[0].mean, stat[0].min, stat[0].max, stat[0].nr_samples);
-		printk(KERN_ERR "  write lat: mean=%llu, min=%llu, max=%llu, samples=%llu\n", stat[1].mean, stat[1].min, stat[1].max, stat[1].nr_samples);
-		return true;
+		print_lat(stat, maxlat);
+		return LAT_EXCEEDED;
+	} else if ((thislat = rwb_sync_issue_lat(rwb)) > maxlat) {
+		printk(KERN_ERR "sync lat %llu > %llu\n", thislat, maxlat);
+		return LAT_EXCEEDED;
 	}
 
-	return false;
-}
-
-static void set_limits(struct rq_wb *rwb, unsigned int depth, u64 now)
-{
-	u64 nsec;
-
-	nsec = 1000000000ULL / int_sqrt((rwb->scale_step + 1) * 100);
-	rwb->end_time = now + nsec;
-
-	if (rwb->scale_step)
-		printk(KERN_ERR "blk-wb: scaling step %u, limit %u%%\n", rwb->scale_step, rwb->perc);
-
-	calc_wb_limits(rwb, depth);
+	return LAT_OK;
 }
 
-static void __update_limits(struct rq_wb *rwb, unsigned int depth, u64 now)
+static int latency_exceeded(struct rq_wb *rwb)
 {
 	struct blk_rq_stat stat[2];
 
 	blk_queue_stat_get(rwb->q, stat);
 
-	/*
-	 * Reset window and steps if we had a good result
-	 */
-	if (!lat_exceeded(rwb, stat)) {
-		rwb->scale_step = 0;
-		if (blk_queue_nonrot(rwb->q))
-			rwb->end_time = now + RWB_NONROT_END_INC;
-		else
-			rwb->end_time = now + RWB_ROT_END_INC;
-		return;
-	}
-
-	/*
-	 * Nothing more to do...
-	 */
-	if (rwb->wb_max == 1)
-		return;
+	return __latency_exceeded(rwb, stat);
+}
 
+static void step_down(struct rq_wb *rwb)
+{
 	/*
-	 * Scale down queue depth by 75%
+	 * Scale down queue depth by 50%
 	 */
 	rwb->scale_step++;
-	rwb->perc = (rwb->perc * 3) / 4;
+	rwb->perc = rwb->perc / 2;
 	if (!rwb->perc)
 		rwb->perc = 1;
 	blk_stat_clear(rwb->q);
-	set_limits(rwb, depth, now);
+	calc_wb_limits(rwb);
 }
 
-static void update_limits(struct rq_wb *rwb)
+static void blk_wb_timer_fn(unsigned long data)
 {
-	u64 now = ktime_to_ns(ktime_get());
+	struct rq_wb *rwb = (struct rq_wb *) data;
+	int status;
 
-	if (now < rwb->end_time)
-		return;
+	/*
+	 * If we exceeded the latency target, step down. If we're known good,
+	 * reset. If we don't know enough to say either exceeded or ok, then
+	 * don't do anything.
+	 */
+	status = latency_exceeded(rwb);
+	switch (status) {
+	case LAT_EXCEEDED:
+		step_down(rwb);
+		printk(KERN_ERR "blk-wb: stepping down: %d\n", rwb->scale_step);
+		break;
+	case LAT_OK:
+		if (rwb->scale_step) {
+			printk(KERN_ERR "blk-wb: reset\n");
+			rwb->scale_step = 0;
+			rwb->perc = 100;
+			calc_wb_limits(rwb);
+		}
+		break;
+	default:
+		break;
+	}
+
+	/*
+	 * Re-arm timer, if we have IO in flight
+	 */
+	if (atomic_read(&rwb->inflight)) {
+		unsigned long expires;
+		u64 nsec;
 
-	__update_limits(rwb, RWB_MAX_DEPTH, now);
+		nsec = 1000000000ULL / int_sqrt((rwb->scale_step + 1) * 100);
+		expires = jiffies + nsecs_to_jiffies(nsec);
+		mod_timer(&rwb->window_timer, expires);
+	}
 }
 
-void blk_wb_update_limits(struct rq_wb *rwb, unsigned int depth)
+
+void blk_wb_update_limits(struct rq_wb *rwb)
 {
 	rwb->scale_step = 0;
-	set_limits(rwb, depth, ktime_to_ns(ktime_get()));
-	wake_up_all(&rwb->wait);
+	calc_wb_limits(rwb);
+
+	if (waitqueue_active(&rwb->wait))
+		wake_up_all(&rwb->wait);
 }
 
 static bool close_io(struct rq_wb *rwb)
@@ -260,8 +297,6 @@ static inline unsigned int get_limit(struct rq_wb *rwb, unsigned long rw)
 {
 	unsigned int limit;
 
-	update_limits(rwb);
-
 	/*
 	 * At this point we know it's a buffered write. If REQ_SYNC is
 	 * set, then it's WB_SYNC_ALL writeback, and we'll use the max
@@ -286,8 +321,8 @@ static inline unsigned int get_limit(struct rq_wb *rwb, unsigned long rw)
 static inline bool may_queue(struct rq_wb *rwb, unsigned long rw)
 {
 	/*
-	 * Inc it here even if disabled, since we'll dec it at completion.
-	 * This only happens if the task was sleeping in __blk_wb_wait(),
+	 * inc it here even if disabled, since we'll dec it at completion.
+	 * this only happens if the task was sleeping in __blk_wb_wait(),
 	 * and someone turned it off at the same time.
 	 */
 	if (!rwb_enabled(rwb)) {
@@ -295,10 +330,7 @@ static inline bool may_queue(struct rq_wb *rwb, unsigned long rw)
 		return true;
 	}
 
-	if (atomic_inc_below(&rwb->inflight, get_limit(rwb, rw)))
-		return true;
-
-	return false;
+	return atomic_inc_below(&rwb->inflight, get_limit(rwb, rw));
 }
 
 /*
@@ -353,6 +385,14 @@ bool blk_wb_wait(struct rq_wb *rwb, struct bio *bio, spinlock_t *lock)
 		goto no_q;
 
 	__blk_wb_wait(rwb, bio->bi_rw, lock);
+
+	if (!timer_pending(&rwb->window_timer)) {
+		unsigned long expires;
+
+		expires = jiffies + nsecs_to_jiffies(RWB_WINDOW_NSEC);
+		mod_timer(&rwb->window_timer, expires);
+	}
+
 	return true;
 
 no_q:
@@ -360,6 +400,26 @@ no_q:
 	return false;
 }
 
+void blk_wb_issue(struct rq_wb *rwb, struct request *rq)
+{
+	if (!rwb_enabled(rwb))
+		return;
+	if (!(rq->cmd_flags & REQ_BUF_INFLIGHT) && !rwb->sync_issue) {
+		rwb->sync_rq = rq;
+		rwb->sync_issue = rq->issue_time;
+	}
+}
+
+void blk_wb_requeue(struct rq_wb *rwb, struct request *rq)
+{
+	if (!rwb_enabled(rwb))
+		return;
+	if (rq == rwb->sync_rq) {
+		rwb->sync_issue = 0;
+		rwb->sync_rq = NULL;
+	}
+}
+
 int blk_wb_init(struct request_queue *q)
 {
 	struct rq_wb *rwb;
@@ -370,18 +430,23 @@ int blk_wb_init(struct request_queue *q)
 
 	atomic_set(&rwb->inflight, 0);
 	init_waitqueue_head(&rwb->wait);
+	setup_timer(&rwb->window_timer, blk_wb_timer_fn, (unsigned long) rwb);
 	rwb->last_comp = rwb->last_issue = jiffies;
 	rwb->bdp_wait = &q->backing_dev_info.wb.dirty_sleeping;
 	rwb->perc = 100;
-	rwb->end_time = ktime_to_ns(ktime_get()) + RWB_NONROT_END_INC;
 	rwb->q = q;
-	blk_wb_update_limits(rwb, blk_queue_depth(q));
+	blk_wb_update_limits(rwb);
 	q->rq_wb = rwb;
 	return 0;
 }
 
 void blk_wb_exit(struct request_queue *q)
 {
-	kfree(q->rq_wb);
-	q->rq_wb = NULL;
+	struct rq_wb *rwb = q->rq_wb;
+
+	if (rwb) {
+		del_timer_sync(&rwb->window_timer);
+		kfree(q->rq_wb);
+		q->rq_wb = NULL;
+	}
 }
diff --git a/block/blk-wb.h b/block/blk-wb.h
index 44e5b21..8e9b7d1 100644
--- a/block/blk-wb.h
+++ b/block/blk-wb.h
@@ -3,6 +3,7 @@
 
 #include <linux/atomic.h>
 #include <linux/wait.h>
+#include <linux/timer.h>
 
 struct rq_wb {
 	/*
@@ -13,8 +14,11 @@ struct rq_wb {
 	unsigned int wb_normal;			/* normal writeback */
 	unsigned int wb_max;			/* max throughput writeback */
 
+	struct timer_list window_timer;
 	unsigned int scale_step;
-	s64 end_time;
+
+	s64 sync_issue;
+	struct request *sync_rq;
 
 	unsigned long last_issue;		/* last non-throttled issue */
 	unsigned long last_comp;		/* last non-throttled comp */
@@ -29,6 +33,8 @@ void blk_wb_done(struct rq_wb *, struct request *);
 bool blk_wb_wait(struct rq_wb *, struct bio *, spinlock_t *);
 int blk_wb_init(struct request_queue *);
 void blk_wb_exit(struct request_queue *);
-void blk_wb_update_limits(struct rq_wb *, unsigned int);
+void blk_wb_update_limits(struct rq_wb *);
+void blk_wb_requeue(struct rq_wb *, struct request *);
+void blk_wb_issue(struct rq_wb *, struct request *);
 
 #endif
-- 
2.8.1

