reverted:
--- b/kernel/sched/fair.c
+++ a/kernel/sched/fair.c
@@ -687,6 +687,8 @@
 	/* when this task enqueue'ed, it will contribute to its cfs_rq's load_avg */
 }
 
+static inline unsigned long cfs_rq_runnable_load_avg(struct cfs_rq *cfs_rq);
+static inline unsigned long cfs_rq_load_avg(struct cfs_rq *cfs_rq);
 #else
 void init_entity_runnable_average(struct sched_entity *se)
 {
@@ -4592,24 +4594,19 @@
 		return wl;
 
 	for_each_sched_entity(se) {
+		long w, W;
-		struct cfs_rq *cfs_rq = se->my_q;
-		long W, w = cfs_rq_load_avg(cfs_rq);
 
+		tg = se->my_q->tg;
-		tg = cfs_rq->tg;
 
 		/*
 		 * W = @wg + \Sum rw_j
 		 */
+		W = wg + calc_tg_weight(tg, se->my_q);
-		W = wg + atomic_long_read(&tg->load_avg);
-
-		/* Ensure \Sum rw_j >= rw_i */
-		W -= cfs_rq->tg_load_avg_contrib;
-		W += w;
 
 		/*
 		 * w = rw_i + @wl
 		 */
+		w = cfs_rq_load_avg(se->my_q) + wl;
-		w += wl;
 
 		/*
 		 * wl = S * s'_i; see (2)
