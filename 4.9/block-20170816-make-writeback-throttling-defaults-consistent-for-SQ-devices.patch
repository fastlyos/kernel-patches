
Manual backport of mainline commit:
https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=8330cdb0fe55c9a9a8e440e56c19233229e0e259

block: Make writeback throttling defaults consistent for SQ devices

When CFQ is used as an elevator, it disables writeback throttling
because they don't play well together. Later when a different elevator
is chosen for the device, writeback throttling doesn't get enabled
again as it should. Make sure CFQ enables writeback throttling (if it
should be enabled by default) when we switch from it to another IO
scheduler.

[HH: carefully backported & tested for 4.9.x + wbt]
Signed-off-by: Jan Kara <jack@suse.cz>
Signed-off-by: Jens Axboe <axboe@fb.com>

diff -rup linux-4.9.44/block/blk-sysfs.c linux-4.9.44-wbt/block/blk-sysfs.c
--- linux-4.9.44/block/blk-sysfs.c	2017-08-16 23:17:42.382479164 +0200
+++ linux-4.9.44-wbt/block/blk-sysfs.c	2017-08-16 23:10:47.358943663 +0200
@@ -774,58 +774,6 @@ struct kobj_type blk_queue_ktype = {
 	.release	= blk_release_queue,
 };
 
-static void blk_wb_stat_get(void *data, struct blk_rq_stat *stat)
-{
-	blk_queue_stat_get(data, stat);
-}
-
-static void blk_wb_stat_clear(void *data)
-{
-	blk_stat_clear(data);
-}
-
-static bool blk_wb_stat_is_current(struct blk_rq_stat *stat)
-{
-	return blk_stat_is_current(stat);
-}
-
-static struct wb_stat_ops wb_stat_ops = {
-	.get		= blk_wb_stat_get,
-	.is_current	= blk_wb_stat_is_current,
-	.clear		= blk_wb_stat_clear,
-};
-
-static void blk_wb_init(struct request_queue *q)
-{
-	struct rq_wb *rwb;
-
-#ifndef CONFIG_BLK_WBT_MQ
-	if (q->mq_ops)
-		return;
-#endif
-#ifndef CONFIG_BLK_WBT_SQ
-	if (q->request_fn)
-		return;
-#endif
-
-	rwb = wbt_init(&q->backing_dev_info, &wb_stat_ops, q);
-
-	/*
-	 * If this fails, we don't get throttling
-	 */
-	if (IS_ERR_OR_NULL(rwb))
-		return;
-
-	if (blk_queue_nonrot(q))
-		rwb->min_lat_nsec = 2000000ULL;
-	else
-		rwb->min_lat_nsec = 75000000ULL;
-
-	wbt_set_queue_depth(rwb, blk_queue_depth(q));
-	wbt_set_write_cache(rwb, test_bit(QUEUE_FLAG_WC, &q->queue_flags));
-	q->rq_wb = rwb;
-}
-
 int blk_register_queue(struct gendisk *disk)
 {
 	int ret;
@@ -865,7 +813,7 @@ int blk_register_queue(struct gendisk *d
 	if (q->mq_ops)
 		blk_mq_register_dev(dev, q);
 
-	blk_wb_init(q);
+	wbt_enable_default(q);
 
 	if (!q->request_fn)
 		return 0;
diff -rup linux-4.9.44/block/blk-wbt.c linux-4.9.44-wbt/block/blk-wbt.c
--- linux-4.9.44/block/blk-wbt.c	2017-08-16 23:17:42.385812526 +0200
+++ linux-4.9.44-wbt/block/blk-wbt.c	2017-08-16 23:10:44.508919268 +0200
@@ -58,6 +58,27 @@ static inline bool rwb_enabled(struct rq
 	return rwb && rwb->wb_normal != 0;
 }
 
+static void blk_wb_stat_get(void *data, struct blk_rq_stat *stat)
+{
+	blk_queue_stat_get(data, stat);
+}
+
+static void blk_wb_stat_clear(void *data)
+{
+	blk_stat_clear(data);
+}
+
+static bool blk_wb_stat_is_current(struct blk_rq_stat *stat)
+{
+	return blk_stat_is_current(stat);
+}
+
+struct wb_stat_ops wb_stat_ops = {
+	.get		= blk_wb_stat_get,
+	.is_current	= blk_wb_stat_is_current,
+	.clear		= blk_wb_stat_clear,
+};
+
 /*
  * Increment 'v', if 'v' is below 'below'. Returns true if we succeeded,
  * false if 'v' + 1 would be bigger than 'below'.
@@ -669,6 +690,38 @@ void wbt_disable_default(struct request_
 }
 EXPORT_SYMBOL_GPL(wbt_disable_default);
 
+void wbt_enable_default(struct request_queue *q)
+{
+	struct rq_wb *rwb;
+
+#ifndef CONFIG_BLK_WBT_MQ
+	if (q->mq_ops)
+		return;
+#endif
+#ifndef CONFIG_BLK_WBT_SQ
+	if (q->request_fn)
+		return;
+#endif
+
+	rwb = wbt_init(&q->backing_dev_info, &wb_stat_ops, q);
+
+	/*
+	 * If this fails, we don't get throttling
+	 */
+	if (IS_ERR_OR_NULL(rwb))
+		return;
+
+	if (blk_queue_nonrot(q))
+		rwb->min_lat_nsec = 2000000ULL;
+	else
+		rwb->min_lat_nsec = 75000000ULL;
+
+	wbt_set_queue_depth(rwb, blk_queue_depth(q));
+	wbt_set_write_cache(rwb, test_bit(QUEUE_FLAG_WC, &q->queue_flags));
+	q->rq_wb = rwb;
+}
+EXPORT_SYMBOL_GPL(wbt_enable_default);
+
 struct rq_wb *wbt_init(struct backing_dev_info *bdi, struct wb_stat_ops *ops,
 		       void *ops_data)
 {
diff -rup linux-4.9.44/block/blk-wbt.h linux-4.9.44-wbt/block/blk-wbt.h
--- linux-4.9.44/block/blk-wbt.h	2017-08-16 23:17:42.385812526 +0200
+++ linux-4.9.44-wbt/block/blk-wbt.h	2017-08-16 22:58:30.845990150 +0200
@@ -117,6 +117,7 @@ void wbt_update_limits(struct rq_wb *);
 void wbt_requeue(struct rq_wb *, struct blk_issue_stat *);
 void wbt_issue(struct rq_wb *, struct blk_issue_stat *);
 void wbt_disable_default(struct request_queue *);
+void wbt_enable_default(struct request_queue *);
 
 void wbt_set_queue_depth(struct rq_wb *, unsigned int);
 void wbt_set_write_cache(struct rq_wb *, bool);
@@ -154,6 +155,9 @@ static inline void wbt_issue(struct rq_w
 static inline void wbt_disable_default(struct request_queue *q)
 {
 }
+static inline void wbt_enable_default(struct request_queue *q)
+{
+}
 static inline void wbt_set_queue_depth(struct rq_wb *rwb, unsigned int depth)
 {
 }
diff -rup linux-4.9.44/block/elevator.c linux-4.9.44-wbt/block/elevator.c
--- linux-4.9.44/block/elevator.c	2017-08-16 23:17:42.392479249 +0200
+++ linux-4.9.44-wbt/block/elevator.c	2017-08-16 23:00:00.816755503 +0200
@@ -40,6 +40,7 @@
 #include <trace/events/block.h>
 
 #include "blk.h"
+#include "blk-wbt.h"
 
 static DEFINE_SPINLOCK(elv_list_lock);
 static LIST_HEAD(elv_list);
@@ -820,6 +821,8 @@ void elv_unregister_queue(struct request
 		kobject_uevent(&e->kobj, KOBJ_REMOVE);
 		kobject_del(&e->kobj);
 		e->registered = 0;
+		/* Re-enable throttling in case elevator disabled it */
+		wbt_enable_default(q);
 	}
 }
 EXPORT_SYMBOL(elv_unregister_queue);
