From 7a1a727d1ff896918b31f8fade8c45fc49a637a1 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Thu, 7 Jun 2018 11:04:13 +0800
Subject: [PATCH] pds: [Sync] 175f0e25abea sched/core: Fix rules for running on online && !active CPUs

---
 kernel/sched/pds.c | 34 ++++++++++++++++++++++------------
 1 file changed, 22 insertions(+), 12 deletions(-)

diff --git a/kernel/sched/pds.c b/kernel/sched/pds.c
index 7b0a0c3248df..f8f2e6fbc47e 100644
--- a/kernel/sched/pds.c
+++ b/kernel/sched/pds.c
@@ -1069,6 +1069,26 @@ void set_task_cpu(struct task_struct *p, unsigned int new_cpu)
 	__set_task_cpu(p, new_cpu);
 }
 
+static inline bool is_per_cpu_kthread(struct task_struct *p)
+{
+	return ((p->flags & PF_KTHREAD) && (1 == p->nr_cpus_allowed));
+}
+
+/*
+ * Per-CPU kthreads are allowed to run on !actie && online CPUs, see
+ * __set_cpus_allowed_ptr() and select_fallback_rq().
+ */
+static inline bool is_cpu_allowed(struct task_struct *p, int cpu)
+{
+	if (!cpumask_test_cpu(cpu, &p->cpus_allowed))
+		return false;
+
+	if (is_per_cpu_kthread(p))
+		return cpu_online(cpu);
+
+	return cpu_active(cpu);
+}
+
 /*
  * This is how migration works:
  *
@@ -1154,16 +1174,8 @@ struct migration_arg {
 static struct rq *__migrate_task(struct rq *rq, struct task_struct *p, int
 				 dest_cpu)
 {
-	if (p->flags & PF_KTHREAD) {
-		if (unlikely(!cpu_online(dest_cpu)))
-			return rq;
-	} else {
-		if (unlikely(!cpu_active(dest_cpu)))
-			return rq;
-	}
-
 	/* Affinity changed (again). */
-	if (unlikely(!cpumask_test_cpu(dest_cpu, &p->cpus_allowed)))
+	if (!is_cpu_allowed(p, dest_cpu))
 		return rq;
 
 	update_rq_clock(rq);
@@ -1413,9 +1425,7 @@ static int select_fallback_rq(int cpu, struct task_struct *p)
 	for (;;) {
 		/* Any allowed, online CPU? */
 		for_each_cpu(dest_cpu, &p->cpus_allowed) {
-			if (!(p->flags & PF_KTHREAD) && !cpu_active(dest_cpu))
-				continue;
-			if (!cpu_online(dest_cpu))
+			if (!is_cpu_allowed(p, dest_cpu))
 				continue;
 			goto out;
 		}
