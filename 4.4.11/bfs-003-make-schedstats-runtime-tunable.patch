
Add necessary BFS bits to work with patch:
"sched/debug: Make schedstats a runtime tunable that is disabled by default"

See: https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=cb2517653fccaf9f9b4ae968c7ee005c1bbacdc5

--- linux-4.4.8.orig/kernel/sched/bfs.c	2016-04-30 03:32:13.130054658 +0200
+++ linux-4.4.8/kernel/sched/bfs.c	2016-04-30 03:43:49.238225722 +0200
@@ -134,6 +134,72 @@
 
 #define RESCHED_US	(100) /* Reschedule if less than this many Î¼s left */
 
+#ifdef CONFIG_SCHEDSTATS
+
+DEFINE_STATIC_KEY_FALSE(sched_schedstats);
+
+#define schedstat_enabled()		static_branch_unlikely(&sched_schedstats)
+
+static void set_schedstats(bool enabled)
+{
+	if (enabled)
+		static_branch_enable(&sched_schedstats);
+	else
+		static_branch_disable(&sched_schedstats);
+}
+
+void force_schedstat_enabled(void)
+{
+	if (!schedstat_enabled()) {
+		pr_info("kernel profiling enabled schedstats, disable via kernel.sched_schedstats.\n");
+		static_branch_enable(&sched_schedstats);
+	}
+}
+
+static int __init setup_schedstats(char *str)
+{
+	int ret = 0;
+	if (!str)
+		goto out;
+
+	if (!strcmp(str, "enable")) {
+		set_schedstats(true);
+		ret = 1;
+	} else if (!strcmp(str, "disable")) {
+		set_schedstats(false);
+		ret = 1;
+	}
+out:
+	if (!ret)
+		pr_warn("Unable to parse schedstats=\n");
+
+	return ret;
+}
+__setup("schedstats=", setup_schedstats);
+
+#ifdef CONFIG_PROC_SYSCTL
+int sysctl_schedstats(struct ctl_table *table, int write,
+			 void __user *buffer, size_t *lenp, loff_t *ppos)
+{
+	struct ctl_table t;
+	int err;
+	int state = static_branch_likely(&sched_schedstats);
+
+	if (write && !capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	t = *table;
+	t.data = &state;
+	err = proc_dointvec_minmax(&t, write, buffer, lenp, ppos);
+	if (err < 0)
+		return err;
+	if (write)
+		set_schedstats(state);
+	return err;
+}
+#endif
+#endif
+
 void print_scheduler_version(void)
 {
 	printk(KERN_INFO "BFS CPU scheduler v0.467 by Con Kolivas.\n");
@@ -1589,7 +1660,8 @@ out_running:
 out_unlock:
 	task_grq_unlock(&flags);
 
-	ttwu_stat(p, cpu, wake_flags);
+	if (schedstat_enabled())
+		ttwu_stat(p, cpu, wake_flags);
 
 	put_cpu();
 
@@ -1617,12 +1689,15 @@ static void try_to_wake_up_local(struct
 	trace_sched_waking(p);
 
 	if (!task_queued(p)) {
-		if (likely(!task_running(p))) {
-			schedstat_inc(rq, ttwu_count);
-			schedstat_inc(rq, ttwu_local);
+		if (schedstat_enabled()) {
+			if (likely(!task_running(p))) {
+				schedstat_inc(rq, ttwu_count);
+				schedstat_inc(rq, ttwu_local);
+			}
 		}
 		ttwu_activate(p, rq, false);
-		ttwu_stat(p, smp_processor_id(), 0);
+		if (schedstat_enabled())
+			ttwu_stat(p, smp_processor_id(), 0);
 		success = true;
 	}
 	ttwu_post_activation(p, rq, success);
--- linux-4.4.8.orig/kernel/sysctl.c	2016-04-30 03:32:13.130054658 +0200
+++ linux-4.4.8/kernel/sysctl.c	2016-04-30 03:49:21.803873310 +0200
@@ -453,6 +453,19 @@ static struct ctl_table kern_table[] = {
 	},
 #endif
 #endif /* !CONFIG_SCHED_BFS */
+#ifdef CONFIG_SCHED_BFS
+#ifdef CONFIG_SCHEDSTATS
+	{
+		.procname	= "sched_schedstats",
+		.data		= NULL,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= sysctl_schedstats,
+		.extra1		= &zero,
+		.extra2		= &one,
+	},
+#endif /* CONFIG_SCHEDSTATS */
+#endif /* CONFIG_SCHED_BFS */
 #ifdef CONFIG_PROVE_LOCKING
 	{
 		.procname	= "prove_locking",
