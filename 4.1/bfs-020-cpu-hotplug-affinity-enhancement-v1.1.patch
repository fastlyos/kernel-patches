From 3b1e6799575e9baada2daa68a4a1face39863640 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Fri, 1 May 2015 09:48:39 +0800
Subject: [PATCH 20/22] bfs: cpu hotplug affinity enhancement, v1.1

This enhancement keep tasks's original cpu affinity intend during cpu
hotplug events. Detail can be checked from
http://cchalpha.blogspot.com/2015/05/about-hotplug-affinity-enhancement.html

v1.1:
Fix compile error when CONFIG_HOTPLUG_CPU is not enabled.
---
 include/linux/sched.h |   6 +--
 kernel/sched/bfs.c    | 110 ++++++++++++++++++--------------------------------
 2 files changed, 43 insertions(+), 73 deletions(-)

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c720480..644f80d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1323,9 +1323,6 @@ struct task_struct {
 #ifdef CONFIG_SMP
 	bool sticky; /* Soft affined flag */
 #endif
-#ifdef CONFIG_HOTPLUG_CPU
-	bool zerobound; /* Bound to CPU0 for hotplug */
-#endif
 	unsigned long rt_timeout;
 #else /* CONFIG_SCHED_BFS */
 	const struct sched_class *sched_class;
@@ -1349,6 +1346,9 @@ struct task_struct {
 	unsigned int policy;
 	int nr_cpus_allowed;
 	cpumask_t cpus_allowed;
+#ifdef CONFIG_SCHED_BFS
+	cpumask_t cpus_allowed_master;
+#endif
 
 #ifdef CONFIG_PREEMPT_RCU
 	int rcu_read_lock_nesting;
diff --git a/kernel/sched/bfs.c b/kernel/sched/bfs.c
index 0ae76ef..ea483cc 100644
--- a/kernel/sched/bfs.c
+++ b/kernel/sched/bfs.c
@@ -675,7 +675,7 @@ static inline void clear_cpuidle_map(int cpu)
 
 static inline bool suitable_idle_cpus(struct task_struct *p)
 {
-	return (cpumask_intersects(&p->cpus_allowed, &grq.cpu_idle_map));
+	return (cpumask_intersects(tsk_cpus_allowed(p), &grq.cpu_idle_map));
 }
 
 static inline bool scaling_rq(struct rq *rq);
@@ -5160,7 +5160,12 @@ void dump_cpu_task(int cpu)
 #ifdef CONFIG_SMP
 void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)
 {
-	cpumask_copy(tsk_cpus_allowed(p), new_mask);
+	cpumask_copy(&p->cpus_allowed_master, new_mask);
+	if (likely(cpumask_and(&p->cpus_allowed,
+			   &p->cpus_allowed_master, cpu_active_mask)))
+		return;
+
+	cpumask_set_cpu(0, &p->cpus_allowed);
 }
 #endif
 
@@ -5364,7 +5369,7 @@ int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)
 
 	rq = task_grq_lock(p, &flags);
 
-	if (cpumask_equal(tsk_cpus_allowed(p), new_mask))
+	if (cpumask_equal(&p->cpus_allowed_master, new_mask))
 		goto out;
 
 	if (!cpumask_intersects(new_mask, cpu_active_mask)) {
@@ -5403,69 +5408,6 @@ out:
 EXPORT_SYMBOL_GPL(set_cpus_allowed_ptr);
 
 #ifdef CONFIG_HOTPLUG_CPU
-extern struct task_struct *cpu_stopper_task;
-/* Run through task list and find tasks affined to the dead cpu, then remove
- * that cpu from the list, enable cpu0 and set the zerobound flag. */
-static void bind_zero(int src_cpu)
-{
-	struct task_struct *p, *t, *stopper;
-	int bound = 0;
-
-	if (src_cpu == 0)
-		return;
-
-	stopper = per_cpu(cpu_stopper_task, src_cpu);
-	do_each_thread(t, p) {
-		if (p != stopper && cpumask_test_cpu(src_cpu, tsk_cpus_allowed(p))) {
-			cpumask_clear_cpu(src_cpu, tsk_cpus_allowed(p));
-			cpumask_set_cpu(0, tsk_cpus_allowed(p));
-			p->zerobound = true;
-			bound++;
-		}
-		clear_sticky(p);
-	} while_each_thread(t, p);
-
-	if (bound) {
-		printk(KERN_INFO "Removed affinity for %d processes to cpu %d\n",
-		       bound, src_cpu);
-	}
-}
-
-/* Find processes with the zerobound flag and reenable their affinity for the
- * CPU coming alive. */
-static void unbind_zero(int src_cpu)
-{
-	int unbound = 0, zerobound = 0;
-	struct task_struct *p, *t;
-
-	if (src_cpu == 0)
-		return;
-
-	do_each_thread(t, p) {
-		if (!p->mm)
-			p->zerobound = false;
-		if (p->zerobound) {
-			unbound++;
-			cpumask_set_cpu(src_cpu, tsk_cpus_allowed(p));
-			/* Once every CPU affinity has been re-enabled, remove
-			 * the zerobound flag */
-			if (cpumask_subset(cpu_possible_mask, tsk_cpus_allowed(p))) {
-				p->zerobound = false;
-				zerobound++;
-			}
-		}
-	} while_each_thread(t, p);
-
-	if (unbound) {
-		printk(KERN_INFO "Added affinity for %d processes to cpu %d\n",
-		       unbound, src_cpu);
-	}
-	if (zerobound) {
-		printk(KERN_INFO "Released forced binding to cpu0 for %d processes\n",
-		       zerobound);
-	}
-}
-
 /*
  * Ensures that the idle task is using init_mm right before its cpu goes
  * offline.
@@ -5482,8 +5424,6 @@ void idle_task_exit(void)
 	}
 	mmdrop(mm);
 }
-#else /* CONFIG_HOTPLUG_CPU */
-static void unbind_zero(int src_cpu) {}
 #endif /* CONFIG_HOTPLUG_CPU */
 
 void sched_set_stop_task(int cpu, struct task_struct *stop)
@@ -5701,6 +5641,36 @@ static void set_rq_offline(struct rq *rq)
 	}
 }
 
+extern struct task_struct *cpu_stopper_task;
+
+/* Run through task list and find tasks affined to the dead cpu, then remove
+ * that cpu from the list, enable cpu0 and set the zerobound flag. */
+static void tasks_cpu_hotplug(int cpu)
+{
+	struct task_struct *p, *t, *stopper;
+	int count = 0;
+
+	if (cpu == 0)
+		return;
+
+	stopper = per_cpu(cpu_stopper_task, cpu);
+	do_each_thread(t, p) {
+		clear_sticky(p);
+		if (p != stopper && cpumask_test_cpu(cpu, &p->cpus_allowed_master)) {
+			count++;
+			if (likely(cpumask_and(tsk_cpus_allowed(p),
+					   &p->cpus_allowed_master,
+					   cpu_active_mask)))
+				continue;
+			cpumask_set_cpu(0, tsk_cpus_allowed(p));
+		}
+	} while_each_thread(t, p);
+
+	if (count) {
+		printk(KERN_INFO "Renew affinity for %d processes to cpu %d\n",
+		       count, cpu);
+	}
+}
 /*
  * migration_call - callback that gets triggered when a CPU is added.
  */
@@ -5728,7 +5698,7 @@ migration_call(struct notifier_block *nfb, unsigned long action, void *hcpu)
 
 			set_rq_online(rq);
 		}
-		unbind_zero(cpu);
+		tasks_cpu_hotplug(cpu);
 		grq.noc = num_online_cpus();
 		cpumask_set_cpu(cpu, &grq.cpu_idle_map);
 		grq_unlock_irqrestore(&flags);
@@ -5749,7 +5719,7 @@ migration_call(struct notifier_block *nfb, unsigned long action, void *hcpu)
 			BUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));
 			set_rq_offline(rq);
 		}
-		bind_zero(cpu);
+		tasks_cpu_hotplug(cpu);
 		grq.noc = num_online_cpus();
 		cpumask_clear_cpu(cpu, &grq.cpu_idle_map);
 		grq_unlock_irqrestore(&flags);
-- 
2.4.6

