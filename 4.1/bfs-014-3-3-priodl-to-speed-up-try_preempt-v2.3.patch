From 52bbb855bbe2ba2e724615e59a5c596186652fd8 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sat, 14 Feb 2015 10:07:35 +0800
Subject: [PATCH 14/22] bfs: [3/3] priodl to speed up try_preempt(), v2.3

Introduced task priodl, which first 8 bits are the task prio and the
last 56 bits are the task deadline(higher 56 bits) to speed up
try_preempt() calculation.

v2
rq's priodl store in grq.rq_priodls array, for 32bits system,
grq.priodl_lock needs to be held for r/w.
v2.1
fix try_preempt should has void return type.

v2.2
priodl fix for 32bit system, v2.2
fix issue that priodl is not set for idle task.

v2.3
fix compile error with non_SMP config.
---
 include/linux/sched.h |   1 +
 kernel/sched/bfs.c    | 110 +++++++++++++++++++++++++++++++++-----------------
 2 files changed, 75 insertions(+), 36 deletions(-)

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ef8e51c..c720480 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1313,6 +1313,7 @@ struct task_struct {
 #ifdef CONFIG_SCHED_BFS
 	int time_slice;
 	u64 deadline;
+	u64 priodl; /* 8bits prio and 56bits deadline for quick processing */
 	struct list_head run_list;
 	u64 last_ran;
 	u64 sched_time; /* sched_clock time spent running */
diff --git a/kernel/sched/bfs.c b/kernel/sched/bfs.c
index dd71560..42a3fac 100644
--- a/kernel/sched/bfs.c
+++ b/kernel/sched/bfs.c
@@ -181,7 +181,11 @@ struct global_rq {
 #ifdef CONFIG_SMP
 	cpumask_t cpu_idle_map;
 	bool idle_cpus;
+#ifndef CONFIG_64BIT
+	raw_spinlock_t priodl_lock;
+#endif
 #endif
+	u64 rq_priodls[NR_CPUS];
 	int noc; /* num_online_cpus stored and updated when it changes */
 	u64 niffies; /* Nanosecond jiffies */
 	unsigned long last_jiffy; /* Last jiffy we updated niffies */
@@ -511,6 +515,31 @@ static inline bool deadline_after(u64 deadline, u64 time)
 	return (deadline > time);
 }
 
+static inline void update_task_priodl(struct task_struct *p)
+{
+	p->priodl = (((u64) (p->prio))<<56) | ((p->deadline)>>8);
+}
+
+#if defined(CONFIG_SMP) && !defined(CONFIG_64BIT)
+static inline void grq_priodl_lock(void)
+{
+	raw_spin_lock(&grq.priodl_lock);
+}
+
+static inline void grq_priodl_unlock(void)
+{
+	raw_spin_unlock(&grq.priodl_lock);
+}
+#else
+static inline void grq_priodl_lock(void)
+{
+}
+
+static inline void grq_priodl_unlock(void)
+{
+}
+#endif
+
 /*
  * A task that is queued but not running will be on the grq run list.
  * A task that is not running or queued will not be on the grq run list.
@@ -569,6 +598,7 @@ static void enqueue_task(struct task_struct *p, struct rq *rq)
 			p->prio = p->normal_prio;
 		else
 			p->prio = NORMAL_PRIO;
+		update_task_priodl(p);
 	}
 	__set_bit(p->prio, grq.prio_bitmap);
 	list_add_tail(&p->run_list, grq.queue + p->prio);
@@ -1287,17 +1317,9 @@ EXPORT_SYMBOL_GPL(kick_process);
  * prio PRIO_LIMIT so it is always preempted.
  */
 static inline bool
-can_preempt(struct task_struct *p, int prio, u64 deadline)
+can_preempt(struct task_struct *p, u64 priodl)
 {
-	/* Better static priority RT task or better policy preemption */
-	if (p->prio < prio)
-		return true;
-	if (p->prio > prio)
-		return false;
-	/* SCHED_NORMAL, BATCH and ISO will preempt based on deadline */
-	if (!deadline_before(p->deadline, deadline))
-		return false;
-	return true;
+	return (p->priodl < priodl);
 }
 
 #ifdef CONFIG_SMP
@@ -1315,9 +1337,8 @@ static inline bool needs_other_cpu(struct task_struct *p, int cpu)
  */
 static void try_preempt(struct task_struct *p, struct rq *this_rq)
 {
-	struct rq *highest_prio_rq = NULL;
-	int cpu, highest_prio;
-	u64 latest_deadline;
+	int cpu, target_cpu;
+	u64 highest_priodl;
 	cpumask_t tmp;
 
 	/*
@@ -1337,34 +1358,28 @@ static void try_preempt(struct task_struct *p, struct rq *this_rq)
 	if (unlikely(!cpumask_and(&tmp, cpu_online_mask, &p->cpus_allowed)))
 		return;
 
-	highest_prio = latest_deadline = 0;
+	target_cpu = cpu = cpumask_first(&tmp);
 
-	for_each_cpu(cpu, &tmp) {
-		struct rq *rq;
-		int rq_prio;
+	grq_priodl_lock();
+	highest_priodl = grq.rq_priodls[cpu];
 
-		rq = cpu_rq(cpu);
-		rq_prio = rq->rq_prio;
-		if (rq_prio < highest_prio)
-			continue;
+	for(;cpu = cpumask_next(cpu, &tmp), cpu < nr_cpu_ids;) {
+		u64 rq_priodl;
 
-		if (rq_prio > highest_prio ||
-		    deadline_after(rq->rq_deadline, latest_deadline)) {
-			latest_deadline = rq->rq_deadline;
-			highest_prio = rq_prio;
-			highest_prio_rq = rq;
+		rq_priodl = grq.rq_priodls[cpu];
+		if (rq_priodl > highest_priodl ) {
+			target_cpu = cpu;
+			highest_priodl = rq_priodl;
 		}
 	}
+	grq_priodl_unlock();
 
-	if (likely(highest_prio_rq)) {
 #ifdef CONFIG_SMT_NICE
-		cpu = cpu_of(highest_prio_rq);
-		if (!smt_should_schedule(p, cpu))
-			return;
+	if (!smt_should_schedule(p, target_cpu))
+		return;
 #endif
-		if (can_preempt(p, highest_prio, highest_prio_rq->rq_deadline))
-			resched_curr(highest_prio_rq);
-	}
+	if (can_preempt(p, highest_priodl))
+		resched_curr(cpu_rq(target_cpu));
 }
 #else /* CONFIG_SMP */
 static inline bool needs_other_cpu(struct task_struct *p, int cpu)
@@ -1376,7 +1391,7 @@ static void try_preempt(struct task_struct *p, struct rq *this_rq)
 {
 	if (p->policy == SCHED_IDLEPRIO)
 		return;
-	if (can_preempt(p, uprq->rq_prio, uprq->rq_deadline))
+	if (can_preempt(p, grq.rq_priodls[0]))
 		resched_curr(uprq);
 }
 #endif /* CONFIG_SMP */
@@ -1680,6 +1695,8 @@ void wake_up_new_task(struct task_struct *p)
 	 */
 	p->prio = rq->curr->normal_prio;
 
+	update_task_priodl(p);
+
 	activate_task(p, rq);
 	trace_sched_wakeup_new(p, 1);
 	if (unlikely(p->policy == SCHED_FIFO))
@@ -2971,6 +2988,7 @@ static void time_slice_expired(struct task_struct *p)
 {
 	p->time_slice = timeslice();
 	p->deadline = grq.niffies + task_deadline_diff(p);
+	update_task_priodl(p);
 #ifdef CONFIG_SMT_NICE
 	if (!p->mm)
 		p->smt_bias = 0;
@@ -3205,6 +3223,11 @@ static inline void set_rq_task(struct rq *rq, struct task_struct *p)
 	rq->rq_last_ran = p->last_ran = rq->clock_task;
 	rq->rq_policy = p->policy;
 	rq->rq_prio = p->prio;
+
+	grq_priodl_lock();
+	grq.rq_priodls[cpu_of(rq)] = p->priodl;
+	grq_priodl_unlock();
+
 #ifdef CONFIG_SMT_NICE
 	rq->rq_mm = p->mm;
 	rq->rq_smt_bias = p->smt_bias;
@@ -3219,6 +3242,12 @@ static void reset_rq_task(struct rq *rq, struct task_struct *p)
 {
 	rq->rq_policy = p->policy;
 	rq->rq_prio = p->prio;
+	rq->rq_deadline = p->deadline;
+
+	grq_priodl_lock();
+	grq.rq_priodls[cpu_of(rq)] = p->priodl;
+	grq_priodl_unlock();
+
 #ifdef CONFIG_SMT_NICE
 	rq->rq_smt_bias = p->smt_bias;
 #endif
@@ -3386,7 +3415,6 @@ static void __sched __schedule(void)
 	if (idle != prev) {
 		/* Update all the information stored on struct rq */
 		prev->time_slice = rq->rq_time_slice;
-		prev->deadline = rq->rq_deadline;
 		check_deadline(prev);
 		prev->last_ran = rq->clock_task;
 
@@ -3697,6 +3725,7 @@ void rt_mutex_setprio(struct task_struct *p, int prio)
 	trace_sched_pi_setprio(p, prio);
 	oldprio = p->prio;
 	p->prio = prio;
+	update_task_priodl(p);
 
 	check_task_changed(rq, p, oldprio);
 
@@ -3747,6 +3776,7 @@ void set_user_nice(struct task_struct *p, long nice)
 	old_static = p->static_prio;
 	p->static_prio = new_static;
 	p->prio = effective_prio(p);
+	update_task_priodl(p);
 
 	if (queued) {
 		enqueue_task(p, rq);
@@ -3921,6 +3951,7 @@ static void __setscheduler(struct rq *rq, struct task_struct *p,
 		p->prio = rt_mutex_get_effective_prio(p, normal_prio(p));
 	else
 		p->prio = normal_prio(p);
+	update_task_priodl(p);
 
 	if (task_running(p)) {
 		reset_rq_task(rq, p);
@@ -4822,8 +4853,10 @@ int __sched yield_to(struct task_struct *p, bool preempt)
 
 	p_rq = task_rq(p);
 	yielded = 1;
-	if (p->deadline > rq->rq_deadline)
+	if (p->deadline > rq->rq_deadline) {
 		p->deadline = rq->rq_deadline;
+		update_task_priodl(p);
+	}
 	p->time_slice += rq->rq_time_slice;
 	rq->rq_time_slice = 0;
 	if (p->time_slice > timeslice())
@@ -5069,6 +5102,8 @@ void init_idle(struct task_struct *idle, int cpu)
 	idle->state = TASK_RUNNING;
 	/* Setting prio to illegal value shouldn't matter when never queued */
 	idle->prio = PRIO_LIMIT;
+	idle->deadline = 0ULL;
+	update_task_priodl(idle);
 #ifdef CONFIG_SMT_NICE
 	idle->smt_bias = 0;
 #endif
@@ -7027,6 +7062,9 @@ void __init sched_init(void)
 	init_defrootdomain();
 	grq.qnr = grq.idle_cpus = 0;
 	cpumask_clear(&grq.cpu_idle_map);
+#ifndef CONFIG_64BIT
+	raw_spin_lock_init(&grq.priodl_lock);
+#endif
 #else
 	uprq = &per_cpu(runqueues, 0);
 #endif
-- 
2.4.6

